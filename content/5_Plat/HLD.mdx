---
id: hld
title: 'Heavy-Light Decomposition'
author: Benjamin Qi, Andrew Cheng
prerequisites:
  - tree-euler
  - RURQ
description: Path and subtree updates and queries.
frequency: 1
---

<FocusProblem problem="sample" />

<FocusProblem problem="sample2" />

## Tutorial

Suppose that you want to support the following operations on a tree:

- Update all nodes along the path from node $x$ to node $y$.

- Find the sum, maximum, minimum (or any other operation that satisfies the
  associative property) along the path from node $x$ to node $y$.

Heavy Light Decomposition (or HLD) supports both operations efficiently.

### Definitions

- A **heavy child** of a node is the child with the largest subtree size rooted
  at the child.
- A **light child** of a node is any child that is not a **heavy child**.
- A **heavy edge** connects a node to its **heavy child**.
- A **light edge** connects a node to any of its **light children**.
- A **heavy path** is the path formed by a collection **heavy edges**.
- A **light path** is the path formed by a collection **light edges**.

### Properties

**Any path from node $x$ to node $y$ on the tree can pass through at most
  $\mathcal{O}(\log N)$ light edges.**

<Spoiler title="Proof">

Consider a path from node $x$ to node $y$, this path can be broken down into a
path from node $x$ to $LCA(x,y)$ and another path from node $y$ to $LCA(x,y)$.

$LCA(x,y)$ must be an ancestor of both $x$ and $y$ by definition. Hence now we
only need to prove that the path from any node to any of its ancestors can only
pass through $\mathcal{O}(\log N)$ light edges to prove the property above.

Suppose that a light edge connects node $x$ and node $y$, where $y$ is a child
of $x$. By the definition of light edge, $y$ must be a light child of $x$. This
means that there has to be another child of $x$ with a larger subtree size than
$y$, or else $y$ will become the heavy child of $x$. Hence the size of the
subtree rooted at $x$ must be at least two times of the size of the subtree
rooted at $y$.

Since the subtree size of a node can not exceed the total number
of nodes $N$, the doubling process can only happen at most $\mathcal{O}(\log N)$
times when traversing up from an arbitrary node to its parent along a light
edge. Given this, we now know that there could be at most $\mathcal{O}(\log N)$
light edges in both the path from $x$ to $LCA(x,y)$ and the path from $LCA(x,y)$
to $y$. Hence there are at most $\mathcal{O}(\log N + \log N)$ =
$\mathcal{O}(\log N)$ light edges in the path from an arbitrary node $x$ to an
arbitrary node $y$.

</Spoiler>

Since a heavy path can only be broken by a light edge (or else the edge will be a part of the heavy
path), we can know that there are at most $\mathcal{O}(\log N)$ heavy paths on
any path from an arbitrary node $x$ to an arbitrary node $y$.

In addition, by using segment trees (or any other RURQ data structure) we can
calculate the value of any consecutive interval on any heavy path in
$\mathcal{O}(\log N)$ time.

Since there are at most $\mathcal{O}(\log N)$ heavy paths and
$\mathcal{O}(\log N)$ light edges, computing the value on the path from node $x$
to node $y$ will take $\mathcal{O}(\log^2 N + \log N)$ = $\mathcal{O}(\log^2 N)$
time. We can answer $Q$ queries in $\mathcal{O}(Q \log^2 N)$ time.

Here's an animation of how the algorithm works:
<video width="960" height="720" controls>
	<source src="/animations/hld.mp4" type="video/mp4"/>
</video>

We can solve the focus problem "Path Queries II" using Heavy Light
Decomposition:

<LanguageSection>

<CPPSection>

```cpp

#include "bits/stdc++.h"
using namespace std;

const int N = 2e5 + 5;
const int D = 19;
const int S = (1 << D);

int n, q, v[N];
vector<int> adj[N];

int sz[N], p[N], dep[N];
int st[S], id[N], tp[N];

void update(int idx, int val) {
	st[idx += n] = val;
	for (idx /= 2; idx; idx /= 2) st[idx] = max(st[2 * idx], st[2 * idx + 1]);
}

int query(int lo, int hi) {
	int ra = 0, rb = 0;
	for (lo += n, hi += n + 1; lo < hi; lo /= 2, hi /= 2) {
		if (lo & 1) ra = max(ra, st[lo++]);
		if (hi & 1) rb = max(rb, st[--hi]);
	}
	return max(ra, rb);
}

int dfs_sz(int cur, int par) {
	sz[cur] = 1;
	p[cur] = par;
	for (int chi : adj[cur]) {
		if (chi == par) continue;
		dep[chi] = dep[cur] + 1;
		p[chi] = cur;
		sz[cur] += dfs_sz(chi, cur);
	}
	return sz[cur];
}

int ct = 1;

void dfs_hld(int cur, int par, int top) {
	id[cur] = ct++;
	tp[cur] = top;
	update(id[cur], v[cur]);
	int h_chi = -1, h_sz = -1;
	for (int chi : adj[cur]) {
		if (chi == par) continue;
		if (sz[chi] > h_sz) {
			h_sz = sz[chi];
			h_chi = chi;
		}
	}
	if (h_chi == -1) return;
	dfs_hld(h_chi, cur, top);
	for (int chi : adj[cur]) {
		if (chi == par || chi == h_chi) continue;
		dfs_hld(chi, cur, chi);
	}
}

int path(int x, int y) {
	int ret = 0;
	while (tp[x] != tp[y]) {
		if (dep[tp[x]] < dep[tp[y]]) swap(x, y);
		ret = max(ret, query(id[tp[x]], id[x]));
		x = p[tp[x]];
	}
	if (dep[x] > dep[y]) swap(x, y);
	ret = max(ret, query(id[x], id[y]));
	return ret;
}

int main() {
	scanf("%d%d", &n, &q);
	for (int i = 1; i <= n; i++) scanf("%d", &v[i]);
	for (int i = 2; i <= n; i++) {
		int a, b;
		scanf("%d%d", &a, &b);
		adj[a].push_back(b);
		adj[b].push_back(a);
	}
	dfs_sz(1, 1);
	dfs_hld(1, 1, 1);
	while (q--) {
		int t;
		scanf("%d", &t);
		if (t == 1) {
			int s, x;
			scanf("%d%d", &s, &x);
			v[s] = x;
			update(id[s], v[s]);
		} else {
			int a, b;
			scanf("%d%d", &a, &b);
			int res = path(a, b);
			printf("%d ", res);
		}
	}
}
```

</CPPSection>

</LanguageSection>

<Resources>
	<Resource
		source="cp-algo"
		title="HLD"
		url="https://cp-algorithms.com/graph/hld.html"
		starred
	>
		For an alternate implementation, see below
	</Resource>
	<Resource
		source="CF"
		title="galen_colin - HLD"
		url="https://codeforces.com/blog/entry/81317"
	>
		blog + video for USACO Cowland. Binary jumping isn't necessary though.
	</Resource>
</Resources>

<Optional title="Tree Queries in O(NQ)">

[This](https://codeforces.com/blog/entry/82211?#comment-691472) is why you don't
set problems where $\Theta(Q\sqrt N\log N)$ is intended ...

</Optional>

## Implementations

<Resources>
	<Resource
		source="CF"
		title="AI-Cash - HLD Implementation"
		url="22072"
		starred
	/>
	<Resource
		source="CF"
		title="adamant - Easiest HLD with subtree queries"
		url="53170"
		starred
	>
		not complete
	</Resource>
	<Resource
		source="Benq"
		title="Complete HLD Implementation"
		url="https://github.com/bqi343/USACO/blob/master/Implementations/content/graphs%20(12)/Trees%20(10)/HLD%20(10.3).h"
		starred
	>
		complete implementation following the above two articles with minor
		modifications
	</Resource>
</Resources>

The heavy-light decomposition algorithm is versatile and can be combined with other data structures like segment trees, binary indexed trees, and range minimum query data structures to solve a wide range of complex tree-related problems.

## Template

<LanguageSection>
<CPPSection>
```cpp

// BeginCodeSnip{Segment Tree}
struct Segtree {
	int n;
	vector<int> st;

	void init(int a, vector<int> &v) {
		n = a;
		st.assign(2 * n, 0);
		for (int i = n; i < 2 * n; ++i) { st[i] = v[i - n]; }
		for (int i = n - 1; i > 0; --i) {
			st[i] = max(st[2 * i], st[2 * i + 1]);
		}
	}

	int query(int a, int b) {
		int s = 0;
		a += n - 1;
		b += n - 1;
		while (a <= b) {
			if (a % 2 == 1) { s = max(s, st[a++]); }
			if (b % 2 == 0) { s = max(s, st[b--]); }
			a /= 2, b /= 2;
		}
		return s;
	}

	void update(int p, int v) {
		for (st[p += n - 1] = v; p > 1; p /= 2) {
			st[p / 2] = max(st[p], st[p ^ 1]);
		}
	}
};
// EndCodeSnip

struct HLD {
	int n;
	vector<int> par, heavy, depth, root, pos;
	Segtree st;

	// DFS to calculate sizes of subtrees and identify heavy children
	int dfs(int u, int p, vector<vector<int>> &adj) {
		int sz = 1, m = 0;  // size of subtree and max size of child subtree
		for (auto v : adj[u]) {
			if (v == p) { continue; }
			depth[v] = depth[u] + 1;
			par[v] = u;  // set parent of the child node
			int t = dfs(v, u, adj);
			if (t > m) {
				heavy[u] = v;  // update heavy child if this child has the
				               // largest subtree
				m = t;         // update max size
			}
			sz += t;
		}
		return sz;
	}

	// construncting the heavy paths
	/**
	We iterate through all the nodes. If a node is not the heavy child of
	another node, then the edge to its parent is a light edge, making the node
	the root of a heavy path. We then continue descending to the heavy children
	until we reach a leaf. This guarantees that all heavy paths are continuous
	segments of the array.
	**/
	HLD(int n, vector<int> &v, vector<vector<int>> &adj) {
		this->n = n;
		heavy.assign(n + 1, 0);
		depth.assign(n + 1, 0);
		root.assign(n + 1, 0);
		pos.assign(n + 1, 0);
		par.assign(n + 1, 0);
		par[1] = 1;
		dfs(1, 1, adj);

		// Decomposition part begins here
		vector<int> a;  // to store the values in the segment tree
		for (int i = 1, curr_pos = 1; i <= n; ++i) {
			if (i == 1 or heavy[par[i]] != i) {
				for (int j = i; j; j = heavy[j]) {
					root[j] = i;
					pos[j] = curr_pos++;  // assign position in the segment tree
					a.push_back(v[j]);
				}
			}
		}
		st.init(n, a);
	}

	// path queries
	int query(int a, int b) {
		int s = 0;
		while (root[a] != root[b]) {
			if (depth[root[a]] > depth[root[b]]) { swap(a, b); }
			s = max(s, st.query(pos[root[b]], pos[b]));
			b = par[root[b]];
		}
		if (depth[a] > depth[b]) { swap(a, b); }
		s = max(s, st.query(pos[a], pos[b]));
		return s;
	}

	// update queries
	void update(int n, int v) { st.update(pos[n], v); }
};
```
</CPPSection>
</LanguageSection>

<Info title = "Note">
The above-mentioned template is about finding the maximum value along the path between $a$ and $b$. For other operations, we can modify our segment tree to find the solution to the queries involving the sum, minimum value between two nodes, etc.
</Info>

At first, we choose an arbitrary root, ususally the node with index $1$ (or $0$ if you use $0$-based indexing), and start a DFS over the tree to find, for each node, its depth and subtree size. After this, we will decompose the tree to eventually create one large segment tree where we can calculate the maximum of the node values over a range of the path, ordering the values of the nodes based on their positions in the path.

The next important step is to find the Lowest Common Ancestor(LCA) of the two nodes for which we will move node $a$ and node $b$ up the tree until both of them are in the same heavy path. We also need to ensure that we are always moving the deeper node, for which we need to swap $a$ and $b$ in case depths of the roots of $a$ and $b$ are different. Once $a$ and $b$ are on the same chain, the higher pointer of $a$ and $b$ is the LCA.

We can calculate maximum of the node values over the path from $a$ to $b$ by first calculating maximum over the ranges of heavy paths that are between $a$ and $b$, and then calculating the maximums of these values and those of the light edges, obtaining the result we seek.

[This](https://github.com/freakin23/CSES/blob/main/Tree%20Algorithms/2134.cpp) is the code for "Path Queries II" using the above template.

<FocusProblem problem = "sample3"/>

Given a rooted tree of $N$ nodes, where each node is uniquely numbered in between $[1...N]$. The node $1$ is the root of the tree. Each node has an integer value which is initially $0$.

You need to perform the following two kinds of queries on the tree:
- $add$ $t$ $value$: Add $value$ to all nodes in subtree rooted at $t$
- $max$ $a$ $b$: Report maximum value on the path from $a$ to $b$

## Intuition

From the question statement, it is clear that this is a Heavy-Light Decomposition(HLD). To handle the query operations, we will combine HLD with Lazy Segment Tree.

The explanation is similar to the one mentioned [above](#template).

## Implementation

<LanguageSection>
<CPPSection>
```cpp
#include <bits/stdc++.h>
using namespace std;

const int N = 1e5 + 9, LG = 18, inf = 1e9 + 9;
// BeginCodeSnip{Lazy Segment Tree}
struct LST {
#define lc (n << 1)
#define rc ((n << 1) | 1)
	int t[4 * N], lazy[4 * N];

	LST() {
		fill(t, t + 4 * N, -inf);
		fill(lazy, lazy + 4 * N, 0);
	}

	inline void push(int n, int b, int e) {
		if (lazy[n] == 0) { return; }
		t[n] = t[n] + lazy[n];
		if (b != e) {
			lazy[lc] = lazy[lc] + lazy[n];
			lazy[rc] = lazy[rc] + lazy[n];
		}
		lazy[n] = 0;
	}

	inline int combine(int a, int b) { return max(a, b); }

	inline void pull(int n) { t[n] = max(t[lc], t[rc]); }

	void build(int n, int b, int e) {
		if (b == e) {
			t[n] = 0;
			return;
		}

		int mid = (b + e) >> 1;
		build(lc, b, mid);
		build(rc, mid + 1, e);
		pull(n);
	}

	void upd(int n, int b, int e, int i, int j, int v) {
		push(n, b, e);
		if (j < b or e < i) { return; }

		if (i <= b and e <= j) {
			lazy[n] += v;
			push(n, b, e);
			return;
		}

		int mid = (b + e) >> 1;
		upd(lc, b, mid, i, j, v);
		upd(rc, mid + 1, e, i, j, v);
		pull(n);
	}

	int query(int n, int b, int e, int i, int j) {
		push(n, b, e);
		if (i > e or b > j) { return -inf; }
		if (i <= b and e <= j) { return t[n]; }
		int mid = (b + e) >> 1;
		return combine(query(lc, b, mid, i, j), query(rc, mid + 1, e, i, j));
	}
} lseg;
// EndCodeSnip

vector<int> adj[N];
int par[N][LG + 1], dep[N], sz[N];

void dfs_sz(int u, int p = 0) {
	par[u][0] = p;
	dep[u] = dep[p] + 1;
	sz[u] = 1;
	for (int i = 1; i <= LG; i++) { par[u][i] = par[par[u][i - 1]][i - 1]; }
	if (p) { adj[u].erase(find(adj[u].begin(), adj[u].end(), p)); }

	for (auto &v : adj[u]) {
		if (v != p) {
			dfs_sz(v, u);
			sz[u] += sz[v];
			if (sz[v] > sz[adj[u][0]]) { swap(v, adj[u][0]); }
		}
	}
}

int lca(int u, int v) {
	if (dep[u] < dep[v]) { swap(u, v); }
	for (int k = LG; k >= 0; k--) {
		if (dep[par[u][k]] >= dep[v]) { u = par[u][k]; }
	}

	if (u == v) { return u; }

	for (int k = LG; k >= 0; k--) {
		if (par[u][k] != par[v][k]) {
			u = par[u][k];
			v = par[v][k];
		}
	}
	return par[u][0];
}

int kth(int u, int k) {
	assert(k >= 0);
	for (int i = 0; i <= LG; i++) {
		if (k & (1 << i)) { u = par[u][i]; }
	}
	return u;
}

int T, head[N], st[N], en[N];
int n;
void dfs_hld(int u) {
	st[u] = ++T;
	for (auto v : adj[u]) {
		head[v] = (v == adj[u][0] ? head[u] : v);
		dfs_hld(v);
	}
	en[u] = T;
}

int query_up(int u, int v) {
	int res = -inf;
	while (head[u] != head[v]) {
		res = max(res, lseg.query(1, 1, n, st[head[u]], st[u]));
		u = par[head[u]][0];
	}
	res = max(res, lseg.query(1, 1, n, st[v], st[u]));
	return res;
}

int query(int u, int v) {
	int l = lca(u, v);
	int res = query_up(u, l);
	if (v != l) { res = max(res, query_up(v, kth(v, dep[v] - dep[l] - 1))); }

	return res;
}

int main() {
	cin >> n;
	for (int i = 1; i < n; i++) {
		int u, v;
		cin >> u >> v;
		adj[u].push_back(v);
		adj[v].push_back(u);
	}
	dfs_sz(1);
	head[1] = 1;
	dfs_hld(1);

	int q;
	cin >> q;
	lseg.build(1, 1, n);
	while (q--) {
		string type;
		int u, v;
		cin >> type >> u >> v;
		if (type == "add") {
			lseg.upd(1, 1, n, st[u], en[u], v);
		} else {
			cout << query(u, v) << '\n';
		}
	}
	return 0;
}
```
</CPPSection>
</LanguageSection>

This way, we can combine HLD with various data structures depending on the query operations.
## Problems

<Problems problems="general" />

<Warning>

"Grass Planting" isn't submittable on the USACO website. Use
[this link](https://www.acmicpc.net/problem/5916) to submit.

</Warning>
